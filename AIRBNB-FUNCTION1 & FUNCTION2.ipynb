{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "from scipy.stats import mode\n",
    "import calendar\n",
    "import random\n",
    "import bisect\n",
    "import xgboost\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import make_scorer\n",
    "%matplotlib inline\n",
    "from scipy.stats import entropy,skew,kurtosis\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(\"train_users_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv(\"test_users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_data=pd.read_csv(\"sessions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(X):\n",
    "    \"\"\"\n",
    "    This function is used for\n",
    "    1.Preprocessing data\n",
    "    2.Feature Engineering\n",
    "      a)Feature Extraction\n",
    "      b)Feature selection\n",
    "    3.Modelling Data\n",
    "    ------------------------------\n",
    "    Function parameters:\n",
    "    -------------------\n",
    "    X: Raw Data(list of DataFrame)\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    predicted_data:predictions(array)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    testdata=X[0]\n",
    "    sessionsdata=X[1]\n",
    "    #loading all the pickle files of models\n",
    "    dic=joblib.load('dictionary_file.pkl')\n",
    "    \n",
    "    #1.DATA PREPROCESSING#\n",
    "    #testdata#\n",
    "    #Replace outliers#\n",
    "    testdata.loc[(testdata[\"age\"]<19) | (testdata[\"age\"]>71),'age']=33.0 #train median age=33.0\n",
    "    \n",
    "    #fill null values#\n",
    "    for i in testdata.columns:\n",
    "        if i!='age':\n",
    "            testdata[i].fillna('NAN',inplace=True)\n",
    "    \n",
    "    #drop columns which are not useful-test data\n",
    "    #dropped date first booking column\n",
    "    testdata.drop(['date_first_booking'],axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    #sessionsdata#\n",
    "    #fill null values#\n",
    "    if isinstance(sessionsdata,pd.DataFrame)==True:\n",
    "        for i in sessionsdata.columns:\n",
    "            if i!='secs_elapsed':\n",
    "                sessionsdata[i].fillna('NAN',inplace=True)#categorical feature\n",
    "            else:\n",
    "                sessionsdata['secs_elapsed'].fillna(0 ,inplace=True)#numerical feature\n",
    "            \n",
    "        #drop rows where userid is null value-sessionsdata\n",
    "        sessionsdata.dropna(subset=['user_id'],inplace=True)\n",
    "    \n",
    "    #2.FEATURE ENGINEERING#\n",
    "    #2.1 FEATURE EXTRACTION#\n",
    "     #if there is sessions data or not\n",
    "    if isinstance(sessionsdata,pd.DataFrame)==False:\n",
    "        mod_session= -1*np.ones((testdata.shape[0],869)) #trained sessionsdata contains 870 features(exclude id of train data)\n",
    "        mod_session=pd.DataFrame(mod_session)\n",
    "        \n",
    "    else:\n",
    "        #sessions data\n",
    "        #if an action is repeated <100,it is replaced with others caetgory\n",
    "        action=joblib.load(dic['action'])\n",
    "        v=[]\n",
    "        for i in action.index:\n",
    "            if(action[i]>100):\n",
    "                v.append(i)\n",
    "    \n",
    "        #replaced as others category\n",
    "        lst=[]\n",
    "        for j in sessionsdata['action']:\n",
    "            if j not in v:\n",
    "                lst.append(\"Others\")\n",
    "            else:\n",
    "                lst.append(j)\n",
    "            \n",
    "        sessionsdata['action']=lst\n",
    "        \n",
    "        #extracted action array/matrix of size (action vector,action frequency) from action column\n",
    "        #where action frequency=no.of times actions appeared and its index is retrieved\n",
    "        #suppose if we have action frequency variable\n",
    "        #index action value counts:#\n",
    "        #4      show        140\n",
    "        #3      fill         10\n",
    "    \n",
    "        #2      search      20\n",
    "        #1     requested     5\n",
    "    \n",
    "        #here considering it as matrix where rows are the users(action_vector) and columns are the indexes  of action frequencies\n",
    "        #index1 index2 index3 index4 -colns\n",
    "     \n",
    "        #eg:15th user actions let say : search show search\n",
    "    \n",
    "        #now 15th row will be  [0 2 0 1]\n",
    "        #search will be incremented twice and show only once\n",
    "        \n",
    "        action_frequency=joblib.load(dic['action_frequency'])\n",
    "        \n",
    "        actions_vector=sessionsdata.groupby('user_id')['action'].apply(lambda x:x.values)\n",
    "        \n",
    "\n",
    "        action_array=np.zeros((len(actions_vector),len(action_frequency)))\n",
    "        \n",
    "        #extracted action type array/matrix of size (action type vector,action type frequency) from action column#\n",
    "        #fill null values with NAN as a seperate category\n",
    "    \n",
    "        sec_log=sessionsdata.groupby('user_id')['secs_elapsed'].apply(lambda x:x.values)\n",
    "    \n",
    "\n",
    "        action_array_sec=np.zeros((len(actions_vector),len(action_frequency)))\n",
    "        for k,i in enumerate(actions_vector):\n",
    "            for l,j in enumerate(i):\n",
    "                action_array[k][action_frequency[j]]+=1\n",
    "                action_array_sec[k][action_frequency[j]]+=sec_log[k][l]\n",
    "        sec_log_array_ac=[]\n",
    "        for i in range(action_array_sec.shape[0]):\n",
    "            v=np.log(1+action_array_sec[i,:])\n",
    "            sec_log_array_ac.append(v)\n",
    "        sec_log_array_ac=np.array(sec_log_array_ac)  \n",
    "\n",
    "        #extracted action detail array/matrix of size (action detail vector,action detail frequency) from action column#\n",
    "    \n",
    "        #fill null values with NAN as a seperate category\n",
    "        \n",
    "        action_detail_frequency=joblib.load(dic['action_detail_frequency'])\n",
    "        \n",
    "        actions_detail_vector=sessionsdata.groupby('user_id')['action_detail'].apply(lambda x:x.values)\n",
    "\n",
    "        actions_detail_array=np.zeros((len(actions_detail_vector),len(action_detail_frequency)))\n",
    "\n",
    "        action_detail_array_sec=np.zeros((len(actions_detail_vector),len(action_detail_frequency)))\n",
    "    \n",
    "        for k,i in enumerate(actions_detail_vector):\n",
    "            for l,j in enumerate(i):\n",
    "                actions_detail_array[k][action_detail_frequency[j]]+=1\n",
    "                action_detail_array_sec[k][action_detail_frequency[j]]+=sec_log[k][l]\n",
    "            \n",
    "        sec_log_array_ac_detail=[]\n",
    "        for i in range(action_detail_array_sec.shape[0]):\n",
    "            v=np.log(1+action_detail_array_sec[i,:])\n",
    "            sec_log_array_ac_detail.append(v)\n",
    "        sec_log_array_ac_detail=np.array(sec_log_array_ac_detail)\n",
    "    \n",
    "            \n",
    "        #extracted device array/matrix of size (device vector,device frequency) from action column#\n",
    "        #fill null values with NAN as a seperate category\n",
    "        \n",
    "        device_frequency=joblib.load(dic['device_frequency'])\n",
    "        \n",
    "        device_vector=sessionsdata.groupby('user_id')['device_type'].apply(lambda x:x.values)\n",
    "        device_array=np.zeros((len(device_vector),len(device_frequency)))\n",
    "        device_array_sec=np.zeros((len(device_vector),len(device_frequency)))\n",
    "    \n",
    "        for k,i in enumerate(device_vector):\n",
    "            for l,j in enumerate(i):\n",
    "                device_array[k][device_frequency[j]]+=1\n",
    "                device_array_sec[k][device_frequency[j]]+=sec_log[k][l]\n",
    "        sec_log_array_device=[]\n",
    "        for i in range(device_array_sec.shape[0]):\n",
    "            v=np.log(1+device_array_sec[i,:])\n",
    "            sec_log_array_device.append(v)\n",
    "        sec_log_array_device=np.array(sec_log_array_device)  \n",
    "            \n",
    "        #extracted action type array/matrix of size (action type vector,action type frequency) from action column#\n",
    "        #fill null values with NAN as a seperate category\n",
    "        \n",
    "        sec_log=sessionsdata.groupby('user_id')['secs_elapsed'].apply(lambda x:x.values)\n",
    "        action_type_frequency = joblib.load(dic['action_type_frequency'])\n",
    "\n",
    "        action_type_vector=sessionsdata.groupby('user_id')['action_type'].apply(lambda x:x.values)\n",
    "\n",
    "        action_type_array=np.zeros((len(action_type_vector),len(action_type_frequency)))\n",
    "        action_type_array_sec=np.zeros((len(action_type_vector),len(action_type_frequency)))\n",
    "        for k,i in enumerate(action_type_vector):\n",
    "            for l,j in enumerate(i):\n",
    "                action_type_array[k][action_type_frequency[j]]+=1\n",
    "                action_type_array_sec[k][action_type_frequency[j]]+=sec_log[k][l]\n",
    "        sec_log_array_ac_type=[]\n",
    "        for i in range(action_type_array_sec.shape[0]):\n",
    "            v=np.log(1+action_type_array_sec[i,:])\n",
    "            sec_log_array_ac_type.append(v)\n",
    "        sec_log_array_ac_type=np.array(sec_log_array_ac_type)\n",
    "        \n",
    "        #unique actions\n",
    "        # no of unique actions done by user\n",
    "\n",
    "        unique_actions=sessionsdata.groupby('user_id')['action'].nunique()\n",
    "        unique_length= sessionsdata.groupby('user_id')['action'].apply(lambda x:len(x.values))  \n",
    "        #obtaining mean,std of value counts of actions done by user,no.of unqiue actions done by user of action column\n",
    "\n",
    "        unique_action_vector=sessionsdata.groupby('user_id')['action'].apply(lambda x:(x.value_counts().values))\n",
    "        unique_action_vector_mean=sessionsdata.groupby('user_id')['action'].apply(lambda x:np.mean(x.value_counts().values))\n",
    "        unique_action_vector_std=sessionsdata.groupby('user_id')['action'].apply(lambda x:np.std(x.value_counts().values))\n",
    "        unique_actions=sessionsdata.groupby('user_id')['action'].apply(lambda x: x.nunique())\n",
    "    \n",
    "        #entropy,skew,kurtosis\n",
    "        unique_action_vector_entropy=sessionsdata.groupby('user_id')['action'].apply(lambda x:entropy(x.value_counts().values))\n",
    "        unique_action_vector_skew=sessionsdata.groupby('user_id')['action'].apply(lambda x:skew(x.value_counts().values))\n",
    "        unique_action_vector_kurtosis=sessionsdata.groupby('user_id')['action'].apply(lambda x:kurtosis(x.value_counts().values))\n",
    "\n",
    "        #obtaining mean,std of value counts of actions done by user,no.of unqiue actions done by user of action detail column\n",
    "\n",
    "        unique_detail_vector=sessionsdata.groupby('user_id')['action_detail'].apply(lambda x:(x.value_counts().values))\n",
    "        unique_detail_vector_mean=sessionsdata.groupby('user_id')['action_detail'].apply(lambda x:np.mean(x.value_counts().values))\n",
    "        unique_detail_vector_std=sessionsdata.groupby('user_id')['action_detail'].apply(lambda x:np.std(x.value_counts().values))\n",
    "        unique_action_detail=sessionsdata.groupby('user_id')['action_detail'].apply(lambda x: x.nunique())\n",
    "    \n",
    "        #entropy,skew,kurtosis\n",
    "        unique_detail_vector_entropy=sessionsdata.groupby('user_id')['action_detail'].apply(lambda x:entropy(x.value_counts().values))\n",
    "        unique_detail_vector_skew=sessionsdata.groupby('user_id')['action_detail'].apply(lambda x:skew(x.value_counts().values))\n",
    "        unique_detail_vector_kurtosis=sessionsdata.groupby('user_id')['action_detail'].apply(lambda x:kurtosis(x.value_counts().values))\n",
    "    \n",
    "        #obtaining mean,std of value counts of actions done by user,no.of unqiue actions done by user of device type column\n",
    "\n",
    "        unique_device_vector=sessionsdata.groupby('user_id')['device_type'].apply(lambda x:(x.value_counts().values))\n",
    "        unique_device_vector_mean=sessionsdata.groupby('user_id')['device_type'].apply(lambda x:np.mean(x.value_counts().values))\n",
    "        unique_device_vector_std=sessionsdata.groupby('user_id')['device_type'].apply(lambda x:np.std(x.value_counts().values))\n",
    "        unique_devices=sessionsdata.groupby('user_id')['device_type'].apply(lambda x: x.nunique())\n",
    "    \n",
    "        #entropy,skew,kurtosis of device type\n",
    "        unique_device_vector_entropy=sessionsdata.groupby('user_id')['device_type'].apply(lambda x:entropy(x.value_counts().values))\n",
    "        unique_device_vector_skew=sessionsdata.groupby('user_id')['device_type'].apply(lambda x:skew(x.value_counts().values))\n",
    "        unique_device_vector_kurtosis=sessionsdata.groupby('user_id')['device_type'].apply(lambda x:kurtosis(x.value_counts().values))\n",
    "    \n",
    "        #obtaining mean,std of value counts of actions done by user,no.of unqiue actions done by user of action type column\n",
    "\n",
    "        unique_ac_type_vector=sessionsdata.groupby('user_id')['action_type'].apply(lambda x:(x.value_counts().values))\n",
    "        unique_ac_type_vector_mean=sessionsdata.groupby('user_id')['action_type'].apply(lambda x:np.mean(x.value_counts().values))\n",
    "        unique_ac_type_vector_std=sessionsdata.groupby('user_id')['action_type'].apply(lambda x:np.std(x.value_counts().values))\n",
    "        unique_ac_type=sessionsdata.groupby('user_id')['action_type'].apply(lambda x: x.nunique()) \n",
    "    \n",
    "        #entropy,skew,kurtosis of action type\n",
    "        unique_ac_type_vector_entropy=sessionsdata.groupby('user_id')['action_type'].apply(lambda x:entropy(x.value_counts().values))\n",
    "        unique_ac_type_vector_skew=sessionsdata.groupby('user_id')['action_type'].apply(lambda x:skew(x.value_counts().values))\n",
    "        unique_ac_type_vector_kurtosis=sessionsdata.groupby('user_id')['action_type'].apply(lambda x:kurtosis(x.value_counts().values))\n",
    "\n",
    "        #extracted log values of sum of secs elapsed column\n",
    "        secs_elapsed_sum=sessionsdata.groupby('user_id')['secs_elapsed'].sum()\n",
    "        secs_elapsed_sum_log=np.log(1+secs_elapsed_sum.values)\n",
    "    \n",
    "         #extracted log values of min of secs elapsed column\n",
    "        #secs_elapsed_min=sessions_data.groupby('user_id')['secs_elapsed'].min()\n",
    "        #secs_elapsed_min_log=np.log(1+secs_elapsed_min.values).astype(int)\n",
    "    \n",
    "        #extracted log values of max of secs elapsed column\n",
    "       # secs_elapsed_max=sessions_data.groupby('user_id')['secs_elapsed'].max()\n",
    "       # secs_elapsed_max_log=np.log(1+secs_elapsed_max.values).astype(int)\n",
    "    \n",
    "        #extracted log values of mean of secs elapsed column\n",
    "        secs_elapsed_mean=sessionsdata.groupby('user_id')['secs_elapsed'].mean()\n",
    "        secs_elapsed_mean_log=np.log(1+secs_elapsed_mean.values)\n",
    "    \n",
    "         #extracted log values of median of secs elapsed column\n",
    "        secs_elapsed_median=sessionsdata.groupby('user_id')['secs_elapsed'].median()\n",
    "        secs_elapsed_median_log=np.log(1+secs_elapsed_median.values)\n",
    "\n",
    "        #extracted log values of std of secs elapsed column\n",
    "        secs_elapsed_std=sessionsdata.groupby('user_id')['secs_elapsed'].std()\n",
    "        secs_elapsed_std_log=np.log(1+secs_elapsed_std.values)\n",
    "\n",
    "    \n",
    "\n",
    "        #extracted bin counts of secs elapsed column\n",
    "        log_secs=sessionsdata.groupby('user_id')['secs_elapsed'].apply(lambda x:np.log(1+x.values).astype(int))\n",
    "\n",
    "        bin_feat=[]\n",
    "        for i in log_secs:\n",
    "            bin_feat.append(np.bincount(i,minlength=15).tolist())\n",
    "    \n",
    "        binfeat_arr=np.array(bin_feat)\n",
    "\n",
    "        cols_bin=[]\n",
    "        for i in range(binfeat_arr.shape[1]):\n",
    "            cols_bin.append(binfeat_arr[:,i])\n",
    "    \n",
    "        #concatenated all the above extracted features\n",
    "        all_arrays=np.hstack((action_array,actions_detail_array,device_array,action_type_array,sec_log_array_ac,sec_log_array_ac_detail,sec_log_array_device,sec_log_array_ac_type))\n",
    "\n",
    "        #creating a dataframe for all the unique actions,action name,secs elapsed done by the user\n",
    "        unique_session=pd.DataFrame()\n",
    "        \n",
    "        \n",
    "        unique_session['id']=unique_actions.index\n",
    "        unique_session['secs_elapsed_sum']=secs_elapsed_sum_log\n",
    "        unique_session['secs_elapsed_mean']=secs_elapsed_mean_log\n",
    "        unique_session['secs_elapsed_mean_sum']=secs_elapsed_sum_log/(secs_elapsed_mean_log).astype(\"float\")\n",
    "        # unique_session['secs_elapsed_min']=secs_elapsed_min_log\n",
    "        # unique_session['secs_elapsed_max']=secs_elapsed_max_log\n",
    "        unique_session['secs_elapsed_median']=secs_elapsed_median_log\n",
    "        unique_session['secs_elapsed_std']=secs_elapsed_std_log\n",
    "    \n",
    "        unique_session['unique_action_vector_mean']=unique_action_vector_mean.values\n",
    "        unique_session['unique_action_vector_std']=unique_action_vector_std.values\n",
    "        unique_session['unique_actions']=unique_actions.values\n",
    "    \n",
    "        unique_session['unique_action_vector_entropy']=unique_action_vector_entropy.values\n",
    "        unique_session['unique_action_vector_skew']=unique_action_vector_skew.values\n",
    "        unique_session['unique_action_vector_kurtosis']=unique_action_vector_kurtosis.values\n",
    "    \n",
    "\n",
    "        unique_session['unique_detail_vector_mean']=unique_detail_vector_mean.values\n",
    "        unique_session['unique_detail_vector_std']=unique_detail_vector_std.values\n",
    "        unique_session['unique_action_detail']=unique_action_detail.values\n",
    "    \n",
    "        unique_session['unique_detail_vector_entropy']=unique_detail_vector_entropy.values\n",
    "        unique_session['unique_detail_vector_skew']=unique_detail_vector_skew.values\n",
    "        unique_session['unique_detail_vector_kurtosis']=unique_detail_vector_kurtosis.values\n",
    "  \n",
    "\n",
    "        unique_session['unique_device_vector_mean']=unique_device_vector_mean.values\n",
    "        unique_session['unique_device_vector_std']=unique_device_vector_std.values\n",
    "        unique_session['unique_devices']=unique_devices.values\n",
    "    \n",
    "        unique_session['unique_device_vector_entropy']=unique_device_vector_entropy.values\n",
    "        unique_session['unique_device_vector_skew']=unique_device_vector_skew.values\n",
    "        unique_session['unique_device_vector_kurtosis']=unique_device_vector_kurtosis.values\n",
    "\n",
    "        unique_session['unique_ac_type_vector_mean']=unique_ac_type_vector_mean.values\n",
    "        unique_session['unique_ac_type_vector_std']=unique_ac_type_vector_std.values\n",
    "        unique_session['unique_ac_type']=unique_ac_type.values\n",
    "    \n",
    "        unique_session['unique_ac_type_vector_entropy']=unique_ac_type_vector_entropy.values\n",
    "        unique_session['unique_ac_type_vector_skew']=unique_ac_type_vector_skew.values\n",
    "        unique_session['unique_ac_type_vector_kurtosis']=unique_ac_type_vector_kurtosis.values\n",
    "        unique_session['len']=unique_length.values\n",
    "        \n",
    "        for i in range(len(cols_bin)):\n",
    "            unique_session[\"bin_secs\"+str(i)]=cols_bin[i]\n",
    "    \n",
    "        all_array=pd.DataFrame(all_arrays) \n",
    "\n",
    "        #final preprocessed sessions dataframe\n",
    "        final_unique_sessions=pd.concat((unique_session,all_array),axis=1)\n",
    "        \n",
    "    #testdata\n",
    "    #timestamp first active#\n",
    "    testdata['timestamp_first_active']=testdata['timestamp_first_active'].apply(str)\n",
    "    testdata['timestamp_first_active']=pd.to_datetime(testdata['timestamp_first_active'])\n",
    "    testdata['day_name']=testdata['timestamp_first_active'].apply(lambda x:calendar.day_name[x.weekday()])\n",
    "    testdata['months']=testdata['timestamp_first_active'].dt.month\n",
    "    testdata['day']=testdata['timestamp_first_active'].dt.day\n",
    "    testdata['year']=testdata['timestamp_first_active'].dt.year\n",
    "    \n",
    "    #date accont created#\n",
    "    testdata['date_account_created']=testdata['date_account_created'].apply(str)\n",
    "    testdata['date_account_created']=pd.to_datetime(testdata['date_account_created'])\n",
    "    testdata['dc_day_name']=testdata['date_account_created'].apply(lambda x:calendar.day_name[x.weekday()])\n",
    "    testdata['dc_months']=testdata['date_account_created'].dt.month\n",
    "    testdata['dc_day']=testdata['date_account_created'].dt.day\n",
    "    testdata['dc_year']=testdata['date_account_created'].dt.year\n",
    "    \n",
    "    \n",
    "    #difference of date acc created-timestamp first active #\n",
    "    c=testdata['date_account_created']\n",
    "    d=testdata['timestamp_first_active']\n",
    "\n",
    "    testdata[\"diff\"]=np.log(1+abs((c.dt.date-d.dt.date).dt.total_seconds()))\n",
    "    testdata['sign_diff']=np.sign((c.dt.date-d.dt.date).dt.total_seconds())\n",
    "    \n",
    "    #quarters-date account created,time stamp first active#\n",
    "    testdata['q_date']=testdata['date_account_created'].dt.quarter\n",
    "    testdata['q_time']=testdata['timestamp_first_active'].dt.quarter \n",
    "    \n",
    "    #drop date acc created and timestamp columns\n",
    "    testdata.drop(['date_account_created','timestamp_first_active'],axis=1,inplace=True)\n",
    "    \n",
    "    #age#\n",
    "    testdata['age_bins']=np.NaN\n",
    "    \n",
    "    #dividing age into 5 groups\n",
    "    testdata.loc[(testdata['age']>=19)&(testdata['age']<=25),'age_bins']='young'\n",
    "    testdata.loc[(testdata['age']>25)&(testdata['age']<=30),'age_bins']='adults'\n",
    "    testdata.loc[(testdata['age']>30)&(testdata['age']<=40),'age_bins']='thirtes'\n",
    "    testdata.loc[(testdata['age']>40)&(testdata['age']<=50),'age_bins']='middle_age'\n",
    "    testdata.loc[testdata['age']>50,'age_bins']='elderly'\n",
    "    \n",
    "    testdata[\"age_bins\"].fillna(\"NAN\",inplace=True)\n",
    "\n",
    "    \n",
    "    #drop age column\n",
    "    testdata.drop(['age'],axis=1,inplace=True)\n",
    "    \n",
    "    #gender#\n",
    "    testdata = testdata.replace('-unknown-', \"NAN\")\n",
    "    \n",
    "    #no.of null values#\n",
    "    testdata['n_null'] = np.array([sum(i == 'NAN') for i in testdata.values])\n",
    "    \n",
    "    categorical_features= ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', \n",
    "                        'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser','age_bins','dc_day_name','day_name']\n",
    "    \n",
    "    #one hotencoding of categorical features\n",
    "    def one_hot(testdata,column):\n",
    "        enc=joblib.load(dic[column])\n",
    "        enc_test=enc.transform(testdata[column].values.reshape(-1,1)).toarray()\n",
    "    \n",
    "        for i in range(enc_test.shape[1]):\n",
    "            testdata[column+str(i)]=enc_test[:,i]\n",
    "    for i in categorical_features:\n",
    "        one_hot(testdata,i)\n",
    "        testdata.drop(i, axis=1,inplace=True)\n",
    "    \n",
    "    #if there is sessions data or not\n",
    "    if isinstance(sessionsdata,pd.DataFrame)==False:\n",
    "        \n",
    "        total_testdata=pd.concat((testdata,mod_session),axis=1)\n",
    "    else:\n",
    "        \n",
    "        total_testdata=pd.merge(testdata,final_unique_sessions,how='left',on='id')\n",
    "    \n",
    "    total_testdata.fillna(-1,inplace=True)\n",
    "    \n",
    "    #drop id column\n",
    "    user_id=total_testdata['id']\n",
    "    total_test_data=total_testdata.drop(['id'],axis=1)\n",
    "    \n",
    "    #scaling total testdata\n",
    "    scaler=joblib.load(dic['scaler'])\n",
    "    \n",
    "    scaled_data=scaler.transform(total_test_data)\n",
    "    \n",
    "    #2.2 FEATURE SELECTION\n",
    "    \n",
    "    #load the feature selection model-xgb\n",
    "    xgb_feature_selection=joblib.load(dic['xgb_feature_selection'])\n",
    "    \n",
    "    # getting feature importance for each\n",
    "    imp_scores = xgb_feature_selection.get_fscore()\n",
    "\n",
    "    # mapping important feature value with features\n",
    "    Imp_Features = np.zeros(scaled_data.shape[1])\n",
    "    for k,v in imp_scores.items():\n",
    "        \n",
    "        Imp_Features[int(k[1:])] = v\n",
    "\n",
    "    # normalization of feature importance    \n",
    "    Imp_Features = Imp_Features/float(np.max(Imp_Features))\n",
    "\n",
    "    # finding thresold to select important feature\n",
    "    score=Imp_Features\n",
    "    thresold = np.sort(score)[::-1][int(len(score)*0.7)] # selecting top 70% fetaures\n",
    "    Imp_Features = score > thresold\n",
    "    \n",
    "    # Re-initilaizing data to keep only important feature\n",
    "    imp_test_data = scaled_data[:, Imp_Features]\n",
    "    \n",
    "    #3.MODELLING#\n",
    "    \n",
    "    #best model-xgb 8000 estimators\n",
    "    xgb_model=joblib.load('xgb_model')\n",
    "    \n",
    "    # evalute model \n",
    "    \n",
    "    predicted_data = xgb_model.predict_proba(imp_test_data)\n",
    "    \n",
    "    return predicted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(X,Y):\n",
    "    \"\"\"\n",
    "    This function is used to compute ndcg metric score for given raw data and true labels\n",
    "    \n",
    "    Function parameters:\n",
    "    ------------------- \n",
    "    X: Raw data (list of dataframe )\n",
    "    Y:True labels(array)\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    score: Metric-Ndcg score(float)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #call fnction1 to get predictions\n",
    "    y_pred=function_1(X)\n",
    "    \n",
    "    #encoded data of labels\n",
    "    enc=joblib.load('label_enc.pkl')\n",
    "    y_true=enc.transform(Y)\n",
    "    \n",
    "    \n",
    "    #https://www.kaggle.com/davidgasquez/ndcg-scorer\n",
    "    def dcg_score(y_true, y_score, k=5):\n",
    "    \n",
    "        \"\"\"\n",
    "        Compute DCG@k Score for an input\n",
    "    \n",
    "        parameters\n",
    "        ----------\n",
    "        y_true <ndarray>  : correct relevance values  \n",
    "        y_score <ndarray> : predicted scores\n",
    "        k <int>           : rank\n",
    "    \n",
    "        returns\n",
    "        -------\n",
    "        dcg_score <float> : DCG@5 score\n",
    "    \n",
    "        \"\"\" \n",
    "    \n",
    "        # Compute releveace values for predictions\n",
    "        order = np.argsort(y_score)[::-1]\n",
    "        y_true = np.take(y_true, order[:k])\n",
    "    \n",
    "        # compute DCG@k for a given point\n",
    "        dcg_numerator   = 2 ** y_true - 1\n",
    "        dcg_denominator = np.log2( np.arange( len(y_true) ) + 2 )\n",
    "        dcg_score = np.sum( dcg_numerator / dcg_denominator )\n",
    "    \n",
    "        return dcg_score\n",
    "    def ndcg_score(ground_truth, predictions, k=5):\n",
    "    \n",
    "        \"\"\"\n",
    "        Compute NDCG@k Score for given inputs\n",
    "    \n",
    "        parameters\n",
    "        ----------\n",
    "        ground_truth <ndarray>  : True Class labels  \n",
    "        predictions <ndarray>   : Predicted probabilities\n",
    "        k <int>                 : Rank\n",
    "    \n",
    "        returns\n",
    "        -------\n",
    "        ndcg_score <float> : NDCG@5 score\n",
    "        \"\"\"    \n",
    "    \n",
    "        # Compute relevance values for ground_truth        \n",
    "        l=joblib.load(\"label_binarizer.pkl\")\n",
    "        T=l.transform(ground_truth)\n",
    "    \n",
    "        # Compute NDCG@k score for all samples\n",
    "        scores = []\n",
    "        for y_true, y_score in zip(T, predictions):\n",
    "        \n",
    "            dcg_k = dcg_score( y_true, y_score, k)\n",
    "            idcg_k = dcg_score( y_true, y_true, k)\n",
    "            ndcg_k = float(dcg_k) / float(idcg_k)\n",
    "            scores.append(ndcg_k)\n",
    "        \n",
    "        # Mean of all scores\n",
    "        ndcg_score=np.mean(scores)\n",
    "    \n",
    "        return ndcg_score\n",
    "    score=ndcg_score(y_true,y_pred)\n",
    "        \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(train_data,sessions_data,sample_size):\n",
    "    \"\"\"\n",
    "    This function is used to generate sample data from train and sessions data to check for predictions\n",
    "    \n",
    "    Function parameters:\n",
    "    -------------------\n",
    "    train_data: Input train/test data\n",
    "    sessions_data: sessions data of users\n",
    "    sample_size: size of samples from input data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X:list of Dataframe of train with or without sessions data\n",
    "    Y:True labels\n",
    "    \"\"\"\n",
    "    #\n",
    "    if isinstance(sessions_data,pd.DataFrame)==True:\n",
    "        \n",
    "        #to identify unique users of train data and sessions data\n",
    "        a=train_data['id'].unique()\n",
    "        b=sessions_data['user_id'].unique()\n",
    "        \n",
    "        random.shuffle(a)\n",
    "        random.shuffle(b)\n",
    "        \n",
    "        #to take same users of traindata and sessions data\n",
    "        lst=[]\n",
    "        for i in b:\n",
    "            if i in a:\n",
    "                if len(lst)==sample_size:\n",
    "                    break\n",
    "                lst.append(i)\n",
    "        \n",
    "        #as train data and sessions have different i/p size/rows/examples we will concatenate one by one and store it in t\n",
    "        df1=train_data.loc[train_data['id']==lst[0]]\n",
    "        df2=sessions_data.loc[sessions_data['user_id']==lst[0]]\n",
    "\n",
    "        for i,j in enumerate(lst):\n",
    "            if i!=0:\n",
    "                a=train_data.loc[train_data['id']==j]\n",
    "                b=sessions_data.loc[sessions_data['user_id']==j]\n",
    "                df1=pd.concat((df1,a),axis=0)\n",
    "                df2=pd.concat((df2,b),axis=0)\n",
    "        t=df1.drop(['country_destination'],axis=1)\n",
    "        X=[t,df2]\n",
    "        Y=df1['country_destination']\n",
    "    else:\n",
    "        #without sessions data\n",
    "        a=train_data.sample(sample_size)\n",
    "        b=a['country_destination']\n",
    "        a=a.drop(['country_destination'],axis=1)\n",
    "        X=[a,[]]\n",
    "        Y=b\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION-1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SINGLE POINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. WITH SESSIONS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with sessions data(for single point)\n",
    "X,Y=data(train_data,sessions_data,1)\n",
    "predictions_with_session=function_1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NDF', 'US', 'other', 'FR', 'GB']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top5 predictions \n",
    "enc=joblib.load('label_enc.pkl')\n",
    "enc.inverse_transform(np.argsort(predictions_with_session[0])[::-1][:5]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.WITHOUT SESSIONS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without sessions data(for single point)\n",
    "X,Y=data(train_data,[],1)\n",
    "predictions_without_session=function_1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NDF', 'US', 'other', 'FR', 'IT']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top5 predictions \n",
    "enc=joblib.load('label_enc.pkl')\n",
    "enc.inverse_transform(np.argsort(predictions_without_session[0])[::-1][:5]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM SET OF POINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. WITH SESSIONS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with sessions data(for random set of points)\n",
    "X,Y=data(train_data,sessions_data,30)\n",
    "predictions_with_session_pts=function_1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample point 0 th top5 predictions ['NDF', 'US', 'other', 'FR', 'GB']\n",
      "sample point 1 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 2 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 3 th top5 predictions ['US', 'NDF', 'GB', 'other', 'FR']\n",
      "sample point 4 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 5 th top5 predictions ['US', 'NDF', 'other', 'FR', 'IT']\n",
      "sample point 6 th top5 predictions ['NDF', 'US', 'IT', 'FR', 'other']\n",
      "sample point 7 th top5 predictions ['NDF', 'US', 'other', 'FR', 'GB']\n",
      "sample point 8 th top5 predictions ['NDF', 'US', 'other', 'IT', 'FR']\n",
      "sample point 9 th top5 predictions ['US', 'NDF', 'other', 'FR', 'ES']\n",
      "sample point 10 th top5 predictions ['US', 'NDF', 'other', 'NL', 'FR']\n",
      "sample point 11 th top5 predictions ['US', 'NDF', 'other', 'FR', 'IT']\n",
      "sample point 12 th top5 predictions ['US', 'NDF', 'other', 'FR', 'IT']\n",
      "sample point 13 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 14 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 15 th top5 predictions ['US', 'NDF', 'other', 'FR', 'ES']\n",
      "sample point 16 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 17 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 18 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 19 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 20 th top5 predictions ['US', 'NDF', 'other', 'IT', 'FR']\n",
      "sample point 21 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 22 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 23 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 24 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 25 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 26 th top5 predictions ['US', 'NDF', 'other', 'FR', 'ES']\n",
      "sample point 27 th top5 predictions ['NDF', 'US', 'other', 'ES', 'IT']\n",
      "sample point 28 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 29 th top5 predictions ['US', 'FR', 'NDF', 'other', 'IT']\n"
     ]
    }
   ],
   "source": [
    "#top5 predictions \n",
    "enc=joblib.load('label_enc.pkl')\n",
    "for i in range(predictions_with_session_pts.shape[0]):\n",
    "    x=enc.inverse_transform(np.argsort(predictions_with_session_pts[i])[::-1][:5]).tolist()\n",
    "    print(\"sample point\",i,\"th\",\"top5 predictions\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.WITHOUT SESSIONS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without sessions data(for random set of points)\n",
    "X,Y=data(train_data,[],30)\n",
    "predictions_without_session_pts=function_1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample point 0 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 1 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 2 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 3 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 4 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 5 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 6 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 7 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 8 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 9 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 10 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 11 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 12 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 13 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 14 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 15 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 16 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 17 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 18 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 19 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 20 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 21 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 22 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 23 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 24 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 25 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 26 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 27 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 28 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 29 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 30 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 31 th top5 predictions ['NDF', 'US', 'FR', 'other', 'IT']\n",
      "sample point 32 th top5 predictions ['US', 'NDF', 'other', 'FR', 'GB']\n",
      "sample point 33 th top5 predictions ['US', 'NDF', 'other', 'FR', 'GB']\n",
      "sample point 34 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 35 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 36 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 37 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 38 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 39 th top5 predictions ['NDF', 'US', 'other', 'FR', 'GB']\n",
      "sample point 40 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 41 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 42 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 43 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 44 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 45 th top5 predictions ['NDF', 'US', 'other', 'FR', 'GB']\n",
      "sample point 46 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 47 th top5 predictions ['NDF', 'US', 'other', 'FR', 'ES']\n",
      "sample point 48 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 49 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 50 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 51 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 52 th top5 predictions ['US', 'NDF', 'other', 'FR', 'IT']\n",
      "sample point 53 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 54 th top5 predictions ['US', 'NDF', 'other', 'FR', 'IT']\n",
      "sample point 55 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 56 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n",
      "sample point 57 th top5 predictions ['NDF', 'US', 'other', 'FR', 'CA']\n",
      "sample point 58 th top5 predictions ['US', 'NDF', 'other', 'FR', 'IT']\n",
      "sample point 59 th top5 predictions ['NDF', 'US', 'other', 'FR', 'IT']\n"
     ]
    }
   ],
   "source": [
    "#top5 predictions \n",
    "enc=joblib.load('label_enc.pkl')\n",
    "for i in range(predictions_without_session_pts.shape[0]):\n",
    "    x=enc.inverse_transform(np.argsort(predictions_without_session_pts[i])[::-1][:5]).tolist()\n",
    "    print(\"sample point\",i,\"th\",\"top5 predictions\",x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTION-2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR SINGLE POINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.WITH SESSIONS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with sessions data(for single point)\n",
    "X,Y=data(train_data,sessions_data,1)\n",
    "function_2(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.WITHOUT SESSIONS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#without sessions data(for single point)\n",
    "X,Y=data(train_data,[],1)\n",
    "function_2(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR RANDOM SET OF POINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.WITH SESSIONS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8548977212525178"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with sessions data(for random set of points)\n",
    "X,Y=data(train_data,sessions_data,30)\n",
    "function_2(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.WITHOUT SESSIONS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8805502758333401"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#without sessions data(for random set of points)\n",
    "X,Y=data(train_data,[],30)\n",
    "function_2(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
